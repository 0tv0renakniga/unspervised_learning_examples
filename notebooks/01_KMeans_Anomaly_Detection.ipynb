{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clustering-Based Anomaly Detection: K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept & Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core idea behind using k-means for anomaly detection is to group data points into clusters. Normal data points will tend to be close to the center of a cluster (centroid), while anomalies will be far from any cluster centroid. The distance of a data point from its nearest cluster centroid can be used as an anomaly score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Clustering**: The k-means algorithm partitions the data into *k* clusters by minimizing the within-cluster sum of squares (WCSS):\
",
    "   $$ \text{WCSS} = \sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2 $$
",
    "   where $C_i$ is the $i$-th cluster and $\mu_i$ is its centroid.\n",
    "2. **Anomaly Score**: For any data point $x$, its anomaly score is its Euclidean distance to the nearest centroid:\n",
    "   $$ \text{Anomaly Score}(x) = \min_{i=1,...,k} ||x - \mu_i||^2 $$
",
    "3. **Thresholding**: A threshold is set on the anomaly scores to classify points as anomalies. A common approach is to use a quantile of the distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate isotropic Gaussian blobs for clustering\n",
    "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Add some outliers\n",
    "outliers = np.array([[0, 5], [6, 2]])\n",
    "X = np.concatenate([X, outliers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fit K-Means Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=10).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculate Anomaly Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance from each point to its closest cluster center\n",
    "distances = np.min(kmeans.transform(X), axis=1)\n",
    "\n",
    "# Set an anomaly threshold (e.g., 95th percentile)\n",
    "threshold = np.quantile(distances, 0.95)\n",
    "\n",
    "# Identify anomalies\n",
    "anomalies = X[distances > threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c='b', label='Normal Data')\n",
    "plt.scatter(anomalies[:, 0], anomalies[:, 1], c='r', label='Anomalies')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='y', marker='*', label='Centroids')\n",
    "plt.title('K-Means Anomaly Detection')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros & Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros\n",
    "- **Simple and easy to implement.**: K-means is a well-understood algorithm.\n",
    "- **Efficient**: It is computationally efficient for large datasets.\n",
    "- **Scalable**: The algorithm scales well to a large number of samples.\n",
    "\n",
    "### Cons\n",
    "- **Requires specifying *k***: The number of clusters must be set beforehand.\n",
    "- **Assumes spherical clusters**: K-means performs poorly on clusters with complex shapes.\n",
    "- **Sensitive to initialization**: The initial placement of centroids can affect the final clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means is a good choice for anomaly detection when:\n",
    "- The clusters are expected to be roughly spherical.\n",
    "- The number of clusters (*k*) is known or can be estimated.\n",
    "- The dataset is large and computational efficiency is important."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}