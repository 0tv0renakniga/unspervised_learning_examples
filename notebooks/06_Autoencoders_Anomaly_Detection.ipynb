{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model-Based Anomaly Detection: Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept & Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders are a type of neural network used for unsupervised learning, particularly for dimensionality reduction and feature learning. For anomaly detection, the idea is to train an autoencoder on normal data. The autoencoder learns to reconstruct the normal data with low error. When the autoencoder is presented with an anomalous data point, it will have a high reconstruction error. This reconstruction error can be used as an anomaly score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Encoder**: The encoder is a neural network that maps the input data *x* to a lower-dimensional representation *z*, called the bottleneck or latent space.\n",
    "   $ z = f(x) $",
    "2. **Decoder**: The decoder is a neural network that reconstructs the input data from the latent representation *z*.\n",
    "   $ x' = g(z) $",
    "3. **Reconstruction Error**: The autoencoder is trained to minimize the reconstruction error, which is the difference between the original input *x* and the reconstructed output *x'*. A common choice for the loss function is the mean squared error (MSE):\\n",
    "   $ L(x, x') = ||x - x'||^2 $",
    "4. **Anomaly Score**: The reconstruction error is used as the anomaly score. A high reconstruction error indicates that the data point is an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generate and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate isotropic Gaussian blobs for clustering\n",
    "X, _ = make_blobs(n_samples=300, centers=1, cluster_std=1.0, random_state=0)\n",
    "\n",
    "# Add some outliers\n",
    "outliers = np.array([[5, 5], [-5, -5]])\n",
    "X_train = X.copy()\n",
    "X = np.concatenate([X, outliers])\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]\n",
    "encoding_dim = 1\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(X_train_scaled, X_train_scaled, epochs=50, batch_size=32, shuffle=True, validation_split=0.1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = autoencoder.predict(X_scaled)\n",
    "mse = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Identify Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold for anomaly detection\n",
    "threshold = np.quantile(mse, 0.95)\n",
    "anomalies = X[mse > threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c='b', label='Normal Data')\n",
    "plt.scatter(anomalies[:, 0], anomalies[:, 1], c='r', label='Anomalies')\n",
    "plt.title('Autoencoder Anomaly Detection')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros & Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros\n",
    "- **Can learn complex patterns**: Autoencoders can capture complex, non-linear relationships in the data.\n",
    "- **Effective for high-dimensional data**: They are well-suited for tasks like image and time-series anomaly detection.\n",
    "- **No assumptions about data distribution**: They are a non-parametric method.\n",
    "\n",
    "### Cons\n",
    "- **Computationally expensive**: Training deep autoencoders can be time-consuming.\n",
    "- **Requires a lot of data**: They typically require a large amount of normal data to learn effectively.\n",
    "- **Can be difficult to tune**: The performance can be sensitive to the network architecture and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders are a good choice for anomaly detection when:\n",
    "- The dataset is large and high-dimensional.\n",
    "- The normal data has complex, non-linear patterns.\n",
    "- There is a sufficient amount of normal data available for training."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
   